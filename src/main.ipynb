{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eHQJ_lqI8uk"
      },
      "source": [
        "<br> Ayman FAHSI | A20440820\n",
        "\n",
        "Mouhammad BAZZI | A20522180\n",
        "\n",
        "\n",
        "CS512 - Spring 2023</br> <h1><br><b><font color='red'>Project</font></br></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3XgzmkkI8uo"
      },
      "source": [
        "## 0. **Libraries Importation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPqpRTgqI8up"
      },
      "outputs": [],
      "source": [
        "# We import the libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.models import vgg19\n",
        "from mmcv.ops import DeformConv2d\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from pytorch_ssim import ssim\n",
        "from pytorch_fid import fid_score\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uik4r1ufI8ur"
      },
      "source": [
        "## 1. **Custom Classes For Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA3XlyBxI8ur"
      },
      "source": [
        "### 1.1. **Nested Deformable Multi-Head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfwQNw9ZI8us"
      },
      "outputs": [],
      "source": [
        "class NestedDMHA(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads):\n",
        "        super(NestedDMHA, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # Pointwise convolution and deformable convolution for Key\n",
        "        self.key_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        # number of output channels for the offset convolution layers number of output channels for the offset convolution layers\n",
        "        # should be 2 * kernel_size * kernel_size (in this case, 2 * 3 * 3 = 18)\n",
        "        self.key_offset = nn.Conv2d(in_channels, 18, kernel_size=3, padding=1)\n",
        "        self.key_conv2 = DeformConv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Pointwise convolution and deformable convolution for Value\n",
        "        self.value_conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "        self.value_offset = nn.Conv2d(in_channels, 18, kernel_size=3, padding=1)\n",
        "        self.value_conv2 = DeformConv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "        # Pointwise convolution for the output Y\n",
        "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        # Softmax for the attention mechanism\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, De, En):\n",
        "        # Compute Key: pointwise convolution, deformable convolution\n",
        "        key_offset = self.key_offset(De)\n",
        "        Key = self.key_conv2(self.key_conv1(De), key_offset)\n",
        "\n",
        "        # Compute Value: pointwise convolution, deformable convolution\n",
        "        value_offset = self.value_offset(De)\n",
        "        Value = self.value_conv2(self.value_conv1(De), value_offset)\n",
        "\n",
        "        # Compute Query:\n",
        "        Query = En\n",
        "\n",
        "        # Compute Key_R\n",
        "        Key_R = torch.matmul(De, Key)\n",
        "\n",
        "        # Compute Value_R\n",
        "        Value_R = torch.matmul(De, Value)\n",
        "\n",
        "        # Compute Query_R\n",
        "        Query_R = torch.matmul(En, Query)\n",
        "\n",
        "        # Perform matrix multiplication of Query and transposed Key\n",
        "        product = torch.matmul(Query_R, Key_R.transpose(-1, -2))\n",
        "\n",
        "        # Apply softmax to the result of matrix multiplication\n",
        "        attention = self.softmax(product)\n",
        "\n",
        "        # Multiply the attention matrix with Value\n",
        "        Y = torch.matmul(attention, Value_R)\n",
        "\n",
        "        # Apply pointwise convolution to Y before returning\n",
        "        Y = self.out_conv(Y)\n",
        "\n",
        "        return Y, De"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KNfCX40I8ut"
      },
      "source": [
        "### 1.2. **Gated Feed Forward Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtAmGLybI8uu"
      },
      "outputs": [],
      "source": [
        "class GFFL(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(GFFL, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        phi = self.conv1(x)\n",
        "        psi = self.conv2(x)\n",
        "        g_psi = F.gelu(psi)\n",
        "\n",
        "        return phi + g_psi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2QssDHkI8uv"
      },
      "source": [
        "### 1.3. **Nested Deformable Multi-Head Attention Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1om3wdmI8uw"
      },
      "outputs": [],
      "source": [
        "class NDMAL(nn.Module):\n",
        "    def __init__(self, in_channels, num_heads):\n",
        "        super(NDMAL, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.dmha1 = NestedDMHA(in_channels, num_heads)\n",
        "        self.dmha2 = NestedDMHA(in_channels, num_heads)\n",
        "\n",
        "        self.norm1 = None\n",
        "        self.norm2 = None\n",
        "        self.norm3 = None\n",
        "\n",
        "        self.gffl = GFFL(in_channels)\n",
        "\n",
        "    def forward(self, De, En):\n",
        "        # First DMHA\n",
        "        Yp, En_p = self.dmha1(De, En)\n",
        "\n",
        "        # Second DMHA\n",
        "        Yu, _ = self.dmha2(Yp, En_p)\n",
        "\n",
        "        # Computing Yd\n",
        "        Yd = De + Yu\n",
        "        if self.norm1 is None:\n",
        "            self.norm1 = nn.LayerNorm(Yd.shape[1:])\n",
        "\n",
        "        # Computing Ye\n",
        "        Ye = En + Yp\n",
        "        if self.norm2 is None:\n",
        "            self.norm2 = nn.LayerNorm(Ye.shape[1:])\n",
        "\n",
        "        # Gated Feed-Forward Layer\n",
        "        Ygffl = self.gffl(Yd)\n",
        "\n",
        "        # Computing Yd'\n",
        "        Yd_prime = Yd + Ygffl\n",
        "        if self.norm3 is None:\n",
        "            self.norm3 = nn.LayerNorm(Yd_prime.shape[1:])\n",
        "\n",
        "        # Concatenating Yd' and Ye\n",
        "        Y_prime = torch.cat([Yd_prime, Ye], dim=1)\n",
        "\n",
        "        return Y_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbxAqkJI8ux"
      },
      "source": [
        "### 1.4. **Gated Convolutional Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2Fo2dOfI8ux"
      },
      "outputs": [],
      "source": [
        "class GatedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n",
        "        super(GatedConv2d, self).__init__()\n",
        "        self.conv_feature = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "        self.conv_gate = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.conv_feature(x)\n",
        "        gates = self.sigmoid(self.conv_gate(x))\n",
        "        return features * gates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8haKuDAOI8uy"
      },
      "source": [
        "### 1.5. **Gated Deconvolutional Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDyI3BJFI8uy"
      },
      "outputs": [],
      "source": [
        "class GatedDeconv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, dilation=1):\n",
        "        super(GatedDeconv2d, self).__init__()\n",
        "        self.deconv_feature = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, dilation=dilation)\n",
        "        self.deconv_gate = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, dilation=dilation)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.deconv_feature(x)\n",
        "        gates = self.sigmoid(self.deconv_gate(x))\n",
        "        return features * gates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGEbkrleI8uz"
      },
      "source": [
        "## 2. **Model Architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN3jOf7NI8uz"
      },
      "outputs": [],
      "source": [
        "class CustomInpainting(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(CustomInpainting, self).__init__()\n",
        "\n",
        "        self.gated_conv_A_1 = GatedConv2d(in_channels, 64, 3, stride=2)\n",
        "        self.gated_conv_A_2 = GatedConv2d(64, 128, 3, stride=2)\n",
        "        self.gated_conv_A_3 = GatedConv2d(128, 256, 3, stride=2)\n",
        "        self.gated_conv_A_4 = GatedConv2d(256, 512, 3, stride=2)\n",
        "\n",
        "        self.gated_conv_B_1 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_2 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_3 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_4 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "\n",
        "        self.ndmal1 = NDMAL(512, num_heads=8)\n",
        "        self.deconv1 = GatedDeconv2d(1024, 256, 3, stride=2)\n",
        "        self.ndmal2 = NDMAL(256, num_heads=8)\n",
        "        self.deconv2 = GatedDeconv2d(512, 128, 3, stride=2)\n",
        "        self.ndmal3 = NDMAL(128, num_heads=8)\n",
        "        self.deconv3 = GatedDeconv2d(256, 64, 3, stride=2)\n",
        "        self.ndmal4 = NDMAL(64, num_heads=8)\n",
        "\n",
        "        self.output_deconv = GatedDeconv2d(128, in_channels-1, 3, 2, output_padding=1)\n",
        "        self.output_act = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1) # We concatenate the mask with the image\n",
        "\n",
        "        x_A1 = self.gated_conv_A_1(x)\n",
        "        x_A1 = torch.nn.functional.leaky_relu(x_A1, negative_slope=0.2)\n",
        "        x_A2 = self.gated_conv_A_2(x_A1)\n",
        "        x_A2 = torch.nn.functional.leaky_relu(x_A2, negative_slope=0.2)\n",
        "        x_A3 = self.gated_conv_A_3(x_A2)\n",
        "        x_A3 = torch.nn.functional.leaky_relu(x_A3, negative_slope=0.2)\n",
        "        x_A4 = self.gated_conv_A_4(x_A3)\n",
        "        x_A4 = torch.nn.functional.leaky_relu(x_A4, negative_slope=0.2)\n",
        "\n",
        "        x_B1 = self.gated_conv_B_1(x_A4)\n",
        "        x_B1 = torch.nn.functional.leaky_relu(x_B1, negative_slope=0.2)\n",
        "        x_B2 = self.gated_conv_B_2(x_B1)\n",
        "        x_B2 = torch.nn.functional.leaky_relu(x_B2, negative_slope=0.2)\n",
        "        x_B3 = self.gated_conv_B_3(x_B2)\n",
        "        x_B3 = torch.nn.functional.leaky_relu(x_B3, negative_slope=0.2)\n",
        "        x_B4 = self.gated_conv_B_4(x_B3)\n",
        "        x_B4 = torch.nn.functional.leaky_relu(x_B4, negative_slope=0.2)\n",
        "\n",
        "        ndmal1_out = self.ndmal1(x_B4, x_A4)\n",
        "        deconv1_out = self.deconv1(ndmal1_out)\n",
        "        ndmal2_out = self.ndmal2(deconv1_out, x_A3)\n",
        "        deconv2_out = self.deconv2(ndmal2_out)\n",
        "        ndmal3_out = self.ndmal3(deconv2_out, x_A2)\n",
        "        deconv3_out = self.deconv3(ndmal3_out)\n",
        "        ndmal4_out = self.ndmal4(deconv3_out, x_A1)\n",
        "\n",
        "        output_deconv = self.output_deconv(ndmal4_out)\n",
        "        output = self.output_act(output_deconv)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxybjscWHWdU"
      },
      "source": [
        "**Stable Model but incomplete**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFgSJUDD0MiW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InpaintingModel(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(InpaintingModel, self).__init__()\n",
        "        \n",
        "        # Encoder layers\n",
        "        self.encoder_conv1 = GatedConv2d(in_channels, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.encoder_conv2 = GatedConv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.encoder_conv3 = GatedConv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.encoder_conv4 = GatedConv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.gated_conv_B_1 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_2 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_3 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "        self.gated_conv_B_4 = GatedConv2d(512, 512, 3, stride=1, padding=1)\n",
        "\n",
        "        \n",
        "        # Decoder layers\n",
        "        self.decoder_conv1 = GatedDeconv2d(1024, 256, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder_conv2 = GatedDeconv2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder_conv3 = GatedDeconv2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.decoder_conv4 = GatedDeconv2d(64, in_channels-1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # NDMAL Layers\n",
        "        self.ndmal1 = NDMAL(512, num_heads=8)\n",
        "        self.ndmal2 = NDMAL(256, num_heads=8)\n",
        "        self.ndmal3 = NDMAL(128, num_heads=8)\n",
        "        self.ndmal4 = NDMAL(64, num_heads=8)\n",
        "        \n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        x = torch.cat([x, mask], dim=1) # We concatenate the mask with the image\n",
        "        # Encoder pass\n",
        "        x_A1 = self.encoder_conv1(x)\n",
        "        x_A1 = torch.nn.functional.leaky_relu(x_A1, negative_slope=0.2)\n",
        "        x_A2 = self.encoder_conv2(x_A1)\n",
        "        x_A2 = torch.nn.functional.leaky_relu(x_A2, negative_slope=0.2)\n",
        "        x_A3 = self.encoder_conv3(x_A2)\n",
        "        x_A3 = torch.nn.functional.leaky_relu(x_A3, negative_slope=0.2)\n",
        "        x_A4 = self.encoder_conv4(x_A3)\n",
        "        x_A4 = torch.nn.functional.leaky_relu(x_A4, negative_slope=0.2)\n",
        "  \n",
        "        x_B1 = self.gated_conv_B_1(x_A4)\n",
        "        x_B1 = torch.nn.functional.leaky_relu(x_B1, negative_slope=0.2)\n",
        "        x_B2 = self.gated_conv_B_2(x_B1)\n",
        "        x_B2 = torch.nn.functional.leaky_relu(x_B2, negative_slope=0.2)\n",
        "        x_B3 = self.gated_conv_B_3(x_B2)\n",
        "        x_B3 = torch.nn.functional.leaky_relu(x_B3, negative_slope=0.2)\n",
        "        x_B4 = self.gated_conv_B_4(x_B3)\n",
        "        x_B4 = torch.nn.functional.leaky_relu(x_B4, negative_slope=0.2)\n",
        "        \n",
        "\n",
        "        # Decoder & NDMAL pass\n",
        "        ndmal_1 = self.ndmal1(x_B4, x_A4)\n",
        "        deconv_1 = self.decoder_conv1(ndmal_1)\n",
        "        deconv_1 = torch.nn.functional.leaky_relu(deconv_1, negative_slope=0.2)\n",
        "        ndmal_2 = self.ndmal2(deconv_1, x_A3)\n",
        "        deconv_2 = self.decoder_conv2(ndmal_2)\n",
        "        deconv_2 = torch.nn.functional.leaky_relu(deconv_2, negative_slope=0.2)\n",
        "        deconv_3 = self.decoder_conv3(deconv_2)\n",
        "        deconv_3 = torch.nn.functional.leaky_relu(deconv_3, negative_slope=0.2)\n",
        "        deconv_4 = self.decoder_conv4(deconv_3)\n",
        "        deconv_4 = self.sigmoid(deconv_4)\n",
        "        \n",
        "        return deconv_4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeJmjpFZI8uz"
      },
      "source": [
        "## 3. **Loss Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqA6QR_TI8u0"
      },
      "outputs": [],
      "source": [
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.vgg = vgg19(pretrained=True).features.eval()\n",
        "        self.vgg = self.vgg.to(device)\n",
        "        self.lambdas = {\n",
        "            'l1': 10,\n",
        "            'adv': 0,\n",
        "            'edge': 2,\n",
        "            'perceptual': 3\n",
        "        }\n",
        "\n",
        "    def adversarial_loss(self, corrupted, target):\n",
        "        pass\n",
        "\n",
        "    def perceptual_loss(self, target, inpainted):\n",
        "        real_features = self.vgg(target)\n",
        "        inpainted_features = self.vgg(inpainted)\n",
        "        return self.l1_loss(real_features, inpainted_features)\n",
        "\n",
        "    def edge_loss(self, target, inpainted):\n",
        "        target = 0.299 * target[:, 0, :, :] + 0.587 * target[:, 1, :, :] + 0.114 * target[:, 2, :, :]\n",
        "        inpainted = 0.299 * inpainted[:, 0, :, :] + 0.587 * inpainted[:, 1, :, :] + 0.114 * inpainted[:, 2, :, :]\n",
        "        sobel_x = torch.Tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).view(1, 1, 3, 3).to(target.device)\n",
        "        sobel_y = torch.Tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]]).view(1, 1, 3, 3).to(target.device)\n",
        "        real_edges_x = nn.functional.conv2d(target, sobel_x, padding=1)\n",
        "        real_edges_y = nn.functional.conv2d(target, sobel_y, padding=1)\n",
        "        inpainted_edges_x = nn.functional.conv2d(inpainted, sobel_x, padding=1)\n",
        "        inpainted_edges_y = nn.functional.conv2d(inpainted, sobel_y, padding=1)\n",
        "        epsilon = 1e-7 # to avoid division by zero\n",
        "        real_edges = torch.sqrt(real_edges_x ** 2 + real_edges_y ** 2 + epsilon)\n",
        "        inpainted_edges = torch.sqrt(inpainted_edges_x ** 2 + inpainted_edges_y ** 2 + epsilon)\n",
        "        return self.l1_loss(real_edges, inpainted_edges)\n",
        "\n",
        "    def forward(self, target, inpainted):\n",
        "        l1_loss = self.l1_loss(target, inpainted)\n",
        "        #adv_loss = self.adversarial_loss(corrupted, inpainted)\n",
        "        perc_loss = self.perceptual_loss(target, inpainted)\n",
        "        edge_loss = self.edge_loss(target, inpainted)\n",
        "        total_loss = self.lambdas['l1'] * l1_loss + self.lambdas['edge'] * edge_loss + self.lambdas['perceptual'] * perc_loss\n",
        "            #+ self.lambdas['adv'] * adv_loss\n",
        "        return total_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EvnoyvI8u3"
      },
      "source": [
        "## 4. **Metrics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIUO8KJkI8u3"
      },
      "source": [
        "### 6.1. **PSNR: Peak Signal to Noise Ratio**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfYcdcp6I8u3"
      },
      "outputs": [],
      "source": [
        "def compute_psnr(img1, img2, max_pixel_value=1.0):\n",
        "    mse = torch.mean((img1 - img2) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    return 20 * torch.log10(max_pixel_value / torch.sqrt(mse))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwcCuqDiI8u4"
      },
      "source": [
        "### 6.2. **SSIM: Structural Similarity Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZWP9JsTI8u4"
      },
      "outputs": [],
      "source": [
        "# SSIM\n",
        "def ssim_manual(img1, img2, window_size=11, k1=0.01, k2=0.03, L=1.0):\n",
        "    C1 = (k1 * L) ** 2\n",
        "    C2 = (k2 * L) ** 2\n",
        "\n",
        "    # Compute the means of img1 and img2\n",
        "    mu1 = F.avg_pool2d(img1, kernel_size=window_size, padding=window_size//2, stride=1)\n",
        "    mu2 = F.avg_pool2d(img2, kernel_size=window_size, padding=window_size//2, stride=1)\n",
        "\n",
        "    mu1_sq = mu1 ** 2\n",
        "    mu2_sq = mu2 ** 2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    # Compute the variances and covariance\n",
        "    sigma1_sq = F.avg_pool2d(img1 * img1, kernel_size=window_size, padding=window_size//2, stride=1) - mu1_sq\n",
        "    sigma2_sq = F.avg_pool2d(img2 * img2, kernel_size=window_size, padding=window_size//2, stride=1) - mu2_sq\n",
        "    sigma12 = F.avg_pool2d(img1 * img2, kernel_size=window_size, padding=window_size//2, stride=1) - mu1_mu2\n",
        "\n",
        "    # Calculate SSIM\n",
        "    ssim_numerator = (2 * mu1_mu2 + C1) * (2 * sigma12 + C2)\n",
        "    ssim_denominator = (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
        "    ssim_value = ssim_numerator / ssim_denominator\n",
        "\n",
        "    # Return the mean SSIM value\n",
        "    return torch.mean(ssim_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqMIitrjI8u4"
      },
      "source": [
        "### 6.3. **L1 Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_rUg7NOI8u4"
      },
      "outputs": [],
      "source": [
        "# L1 loss\n",
        "def l1_loss(image1, image2):\n",
        "    return F.l1_loss(image1, image2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0BDD_73I8u0"
      },
      "source": [
        "## 5. **Data Importation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBUFGIRdI8u1"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Global variables #\n",
        "####################\n",
        "\n",
        "# Device to use for training\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Batch size\n",
        "batch_size = 1\n",
        "# Image paths\n",
        "image_folder = '/content/drive/MyDrive/testFaces/test/corrupt train'\n",
        "mask_folder = '/content/drive/MyDrive/testFaces/test/masks'\n",
        "target_folder = '/content/drive/MyDrive/testFaces/test/target train'\n",
        "\n",
        "# For data split\n",
        "training_percentage = 0.6\n",
        "validation_percentage = 0.2\n",
        "\n",
        "# Random state\n",
        "random_state = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJLttYVlI8u1"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, rgb=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_files = os.listdir(root_dir)\n",
        "        self.rgb = rgb\n",
        "        self.image_numbers = [int(file.split(\".\")[0]) for file in self.image_files]\n",
        "        self.sorted_image_files = [x for _, x in sorted(zip(self.image_numbers, self.image_files))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    # I want to return to tensors, one for the image and one for the images number (id)\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image path\n",
        "        img_path = os.path.join(self.root_dir, self.sorted_image_files[idx])\n",
        "        if self.rgb:\n",
        "            # Open the image using openCV\n",
        "            image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
        "            # Convert the image from BGR to RGB\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            # Open the image using openCV\n",
        "            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        # Resize the image to 512x512\n",
        "        image = cv2.resize(image, (512, 512))\n",
        "        # Normalize the image\n",
        "        image = image / 255.0\n",
        "        # Convert the image to a tensor\n",
        "        if self.rgb:\n",
        "            image = torch.from_numpy(image).float().permute(2, 0, 1)\n",
        "        else:\n",
        "            image = torch.from_numpy(image).float().unsqueeze(0)\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "# Instantiate the dataset\n",
        "image_dataset = ImageDataset(root_dir=image_folder, rgb=True)\n",
        "mask_dataset = ImageDataset(root_dir=mask_folder, rgb=False)\n",
        "target_dataset = ImageDataset(root_dir=target_folder, rgb=True)\n",
        "\n",
        "\n",
        "# We split the data into training, validation and test sets\n",
        "train_size = int(training_percentage * len(image_dataset))\n",
        "val_size = int(validation_percentage * len(image_dataset))\n",
        "test_size = len(image_dataset) - train_size - val_size\n",
        "\n",
        "\n",
        "train_image, val_image, test_image = random_split(image_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(random_state))\n",
        "train_mask, val_mask, test_mask = random_split(mask_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(random_state))\n",
        "train_target, val_target, test_target = random_split(target_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(random_state))\n",
        "\n",
        "# We create the data loaders\n",
        "train_loader = DataLoader(train_image, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_image, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_image, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# We create the mask loaders\n",
        "train_mask_loader = DataLoader(train_mask, batch_size=batch_size, shuffle=False)\n",
        "val_mask_loader = DataLoader(val_mask, batch_size=batch_size, shuffle=False)\n",
        "test_mask_loader = DataLoader(test_mask, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# We create the target loaders\n",
        "train_target_loader = DataLoader(train_target, batch_size=batch_size, shuffle=False)\n",
        "val_target_loader = DataLoader(val_target, batch_size=batch_size, shuffle=False)\n",
        "test_target_loader = DataLoader(test_target, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# We will merge the train and validation sets to create a new training set\n",
        "train_val_loader = DataLoader(train_image + val_image, batch_size=batch_size, shuffle=False)\n",
        "train_val_mask_loader = DataLoader(train_mask + val_mask, batch_size=batch_size, shuffle=False)\n",
        "train_val_target_loader = DataLoader(train_target + val_target, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "L0-UoOXrafaD",
        "outputId": "eb50224a-cddd-4e17-e1c4-ae50e77a2109"
      },
      "outputs": [],
      "source": [
        "# Plot one image\n",
        "fig, ax = plt.subplots(1, 3, figsize=(15, 15))\n",
        "for i, (image, mask, target) in enumerate(zip(train_val_loader, train_val_mask_loader, train_val_target_loader)):\n",
        "    ax[0].imshow(image[0].squeeze().numpy().transpose(1, 2, 0))\n",
        "    ax[0].set_title(\"Image\")\n",
        "    ax[1].imshow(mask[0].squeeze().numpy(), cmap=\"gray\")\n",
        "    ax[1].set_title(\"Mask\")\n",
        "    ax[2].imshow(target[0].squeeze().numpy().transpose(1, 2, 0))\n",
        "    ax[2].set_title(\"Target\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPCO4j5gI8u1"
      },
      "source": [
        "## 6. **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZxukFQNI8u2"
      },
      "outputs": [],
      "source": [
        "####################\n",
        "# Global variables #\n",
        "####################\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 12\n",
        "# Learning rate\n",
        "learning_rate = 0.00008\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.5\n",
        "# Beta2 hyperparam for Adam optimizers\n",
        "beta2 = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6U2fCxtUI8u2",
        "outputId": "51cca56e-b147-4bee-b292-b5ec2787b70f"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "model = InpaintingModel(in_channels=4)\n",
        "\n",
        "# Move the model to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss criterion and optimizer\n",
        "criterion = CustomLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
        "\n",
        "# we want to check if device is cuda or cpu\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "# Training loop\n",
        "loss_list = []\n",
        "psnr_list = []\n",
        "ssim_list = []\n",
        "l1_list = []\n",
        "for epoch in range(num_epochs):\n",
        "    time_start = time.time()\n",
        "    count_batch = 0\n",
        "    loss_epoch = []\n",
        "    psnr_epoch = []\n",
        "    ssim_epoch = []\n",
        "    l1_epoch = []\n",
        "    for (input_image, mask, target) in zip(train_val_loader, train_val_mask_loader, train_val_target_loader):\n",
        "        # Move the tensors to the device\n",
        "        input_image = input_image.to(device)\n",
        "        mask = mask.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        inpainted_images = model(input_image, mask)\n",
        "\n",
        "        # Plot the images\n",
        "        if count_batch % 200 == 0:\n",
        "            plt.figure(figsize=(20, 20))\n",
        "            plt.subplot(1, 4, 1)\n",
        "\n",
        "            if is_cuda:\n",
        "                plt.imshow(input_image[0].permute(1, 2, 0).detach().cpu())\n",
        "            else:\n",
        "                plt.imshow(input_image[0].permute(1, 2, 0))\n",
        "            plt.subplot(1, 4, 2)\n",
        "            if is_cuda:\n",
        "                plt.imshow(mask[0][0].detach().cpu(), cmap=\"gray\")\n",
        "            else:\n",
        "                plt.imshow(mask[0][0], cmap=\"gray\")\n",
        "            plt.subplot(1, 4, 3)\n",
        "            if is_cuda:\n",
        "                plt.imshow(inpainted_images[0].permute(1, 2, 0).detach().cpu())\n",
        "            else:\n",
        "                plt.imshow(inpainted_images[0].permute(1, 2, 0).detach().cpu())\n",
        "            plt.subplot(1, 4, 4)\n",
        "            if is_cuda:\n",
        "                plt.imshow(target[0].permute(1, 2, 0).detach().cpu())\n",
        "            else:\n",
        "                plt.imshow(target[0].permute(1, 2, 0))\n",
        "            plt.show()\n",
        "\n",
        "        # Calculate the losses\n",
        "        loss = criterion(target, inpainted_images)\n",
        "        loss_epoch.append(loss.item())\n",
        "\n",
        "        # Calculate the metrics\n",
        "        psnr = compute_psnr(target, inpainted_images)\n",
        "        ssim = ssim_manual(target, inpainted_images)\n",
        "        l1 = l1_loss(target, inpainted_images)\n",
        "        psnr_epoch.append(psnr.item())\n",
        "        ssim_epoch.append(ssim.item())\n",
        "        l1_epoch.append(l1.item())\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the percentage of batches done each 10 batches\n",
        "        if count_batch % 200 == 0:\n",
        "            print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
        "                    \"Batch: {}...\".format(count_batch),\n",
        "                    \"Loss: {:.4f}...\".format(np.mean(loss_epoch)),\n",
        "                    \"PSNR: {:.4f}...\".format(np.mean(psnr_epoch)),\n",
        "                    \"SSIM: {:.4f}...\".format(np.mean(ssim_epoch)),\n",
        "                    \"L1: {:.4f}\".format(np.mean(l1_epoch)))\n",
        "        count_batch += 1\n",
        "    \n",
        "    # Calculate the loss and metrics for the epoch\n",
        "    loss_list.append(np.mean(loss_epoch))\n",
        "    psnr_list.append(np.mean(psnr_epoch))\n",
        "    ssim_list.append(np.mean(ssim_epoch))\n",
        "    l1_list.append(np.mean(l1_epoch))\n",
        "\n",
        "\n",
        "    time_end = time.time()\n",
        "    print(\"Epoch: {}/{}...\".format(epoch+1, num_epochs),\n",
        "            \"Loss: {:.4f}...\".format(np.mean(loss_epoch)),\n",
        "            \"PSNR: {:.4f}...\".format(np.mean(psnr_epoch)),\n",
        "            \"SSIM: {:.4f}...\".format(np.mean(ssim_epoch)),\n",
        "            \"L1: {:.4f}...\".format(np.mean(l1_epoch)),\n",
        "            \"Time: {:.4f}\".format(time_end - time_start))\n",
        "    \n",
        "\n",
        "# Plot the loss and metrics on a subplot\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 15))\n",
        "ax[0].plot(loss_list)\n",
        "ax[0].set_title(\"Loss\")\n",
        "ax[1].plot(psnr_list)\n",
        "ax[1].set_title(\"PSNR\")\n",
        "ax[2].plot(ssim_list)\n",
        "ax[2].set_title(\"SSIM\")\n",
        "ax[3].plot(l1_list)\n",
        "ax[3].set_title(\"L1\")\n",
        "\n",
        "    \n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"inpainting_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "id": "uvhz8bmJI8u3",
        "outputId": "c4332c3c-bc55-47a3-e48f-d28d0b0cc691"
      },
      "outputs": [],
      "source": [
        "# Now we want to evaluate the model on the test set using the trained model\n",
        "\n",
        "# Load the trained model\n",
        "# !!!!!!!!!!!!!\n",
        "#model.load_state_dict(torch.load(\"inpainting_model.pth\"))\n",
        "# !!!!!!!!!!!!!\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Test loop\n",
        "test_loss_list = []\n",
        "test_psnr_list = []\n",
        "test_ssim_list = []\n",
        "test_l1_list = []\n",
        "inpainted_images_list = []\n",
        "\n",
        "for (input_image, mask, target) in zip(test_loader, test_mask_loader, test_target_loader):\n",
        "    # Move the tensors to the device\n",
        "    input_image = input_image.to(device)\n",
        "    mask = mask.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    start_time = time.time()\n",
        "    inpainted_images = model(input_image, mask)\n",
        "    end_time = time.time()\n",
        "    print(\"Time: {:.4f}\".format(end_time - start_time))\n",
        "    inpainted_images_list.append(inpainted_images)\n",
        "\n",
        "    # Calculate the losses\n",
        "    loss = criterion(target, inpainted_images)\n",
        "\n",
        "    # Calculate the metrics\n",
        "    psnr = compute_psnr(target, inpainted_images)\n",
        "    ssim = ssim_manual(target, inpainted_images)\n",
        "    l1 = l1_loss(target, inpainted_images)\n",
        "    test_loss_list.append(loss.item())\n",
        "    test_psnr_list.append(psnr.item())\n",
        "    test_ssim_list.append(ssim.item())\n",
        "    test_l1_list.append(l1.item())\n",
        "\n",
        "\n",
        "\n",
        "# Print the test metrics\n",
        "print(\"Test Loss: {:.4f}\".format(np.mean(test_loss_list)),\n",
        "        \"Test PSNR: {:.4f}\".format(np.mean(test_psnr_list)),\n",
        "        \"Test SSIM: {:.4f}\".format(np.mean(test_ssim_list)),\n",
        "        \"Test L1: {:.4f}\".format(np.mean(test_l1_list)))\n",
        "\n",
        "# Plot 2 images from the test set\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(input_image[0].permute(1, 2, 0).detach().cpu())\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(mask[0][0].detach().cpu(), cmap=\"gray\")\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(inpainted_images[0].permute(1, 2, 0).detach().cpu())\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(target[0].permute(1, 2, 0).detach().cpu())\n",
        "plt.show()\n",
        "\n",
        "# Plot 2 images from the test set\n",
        "plt.figure(figsize=(20, 20))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(input_image[1].permute(1, 2, 0).detach().cpu())\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(mask[1][0].detach().cpu(), cmap=\"gray\")\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(inpainted_images[1].permute(1, 2, 0).detach().cpu())\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(target[1].permute(1, 2, 0).detach().cpu())\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z3XgzmkkI8uo",
        "uik4r1ufI8ur",
        "oA3XlyBxI8ur",
        "5KNfCX40I8ut",
        "A2QssDHkI8uv",
        "0WbxAqkJI8ux",
        "8haKuDAOI8uy",
        "NGEbkrleI8uz",
        "xeJmjpFZI8uz",
        "g0BDD_73I8u0",
        "IPCO4j5gI8u1",
        "G7EvnoyvI8u3"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
